{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "gxcwyogwhnddewgyyjjo",
   "authorId": "3653328329535",
   "authorName": "NAMAY",
   "authorEmail": "namay.sehgal.ug23@nsut.ac.in",
   "sessionId": "e2982669-7626-4d52-b831-fc87001820a9",
   "lastEditTime": 1737502903558
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "resultHeight": 111
   },
   "source": "import streamlit as st\nfrom snowflake.core import Root\nfrom snowflake.cortex import Complete\nfrom snowflake.snowpark.context import get_active_session\n\n# Initialize session and root\nsession = get_active_session()\nroot = Root(session)\n\nMODELS = [\"mistral-large2\"]\n\n# Enhanced page config\nst.set_page_config(\n    page_title=\"ReflectionsAI\",\n    page_icon=\"üìñ\",\n    layout=\"centered\",\n    initial_sidebar_state=\"expanded\"\n)\n\n# Updated purple theme CSS\nst.markdown(\"\"\"\n    <style>\n    .stApp {\n        background: linear-gradient(135deg, #f5f0ff 0%, #e0d0ff 100%);\n    }\n    .main {\n        background-color: rgba(255, 255, 255, 0.95);\n        padding: 2rem;\n        border-radius: 1rem;\n        box-shadow: 0 4px 6px rgba(128, 0, 255, 0.1);\n    }\n    .stButton>button {\n        background: linear-gradient(135deg, #9c27b0 0%, #673ab7 100%);\n        color: white;\n        border: none;\n        border-radius: 0.5rem;\n        padding: 0.5rem 1rem;\n        transition: all 0.3s ease;\n    }\n    .stButton>button:hover {\n        transform: translateY(-2px);\n        box-shadow: 0 4px 12px rgba(128, 0, 255, 0.2);\n    }\n    .stTextArea>div>div>textarea {\n        border-radius: 0.5rem;\n        border: 2px solid #e0d0ff;\n    }\n    .stTextArea>div>div>textarea:focus {\n        border-color: #9c27b0;\n        box-shadow: 0 0 0 2px rgba(156, 39, 176, 0.2);\n    }\n    .css-1d391kg {\n        background: linear-gradient(135deg, #9c27b0 0%, #673ab7 100%);\n    }\n    .stTabs [data-baseweb=\"tab-list\"] {\n        gap: 1rem;\n    }\n    .stTabs [data-baseweb=\"tab\"] {\n        background-color: #f5f0ff;\n        border-radius: 0.5rem;\n        padding: 0.5rem 1rem;\n        color: #673ab7;\n    }\n    .stTabs [aria-selected=\"true\"] {\n        background-color: #9c27b0;\n        color: white;\n    }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n\ndef init_messages():\n    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\ndef init_service_metadata():\n    \"\"\"\n    Initialize the session state for cortex search service metadata. \n    Query the available cortex search services from the Snowflake session \n    and store their names and search columns in the session state.\n    \"\"\"\n    # Check if service metadata is already initialized in session state\n    if \"service_metadata\" not in st.session_state:\n        # Run the SQL query to fetch available Cortex search services\n        services = session.sql(\"SHOW CORTEX SEARCH SERVICES;\").collect()\n\n        # Initialize an empty list to store service metadata\n        service_metadata = []\n\n        # Check if services were found\n        if services:\n            # Loop through each service and fetch its search column\n            for s in services:\n                svc_name = s[\"name\"]\n                try:\n                    # Query the search column for the current service\n                    svc_search_col = session.sql(f\"DESC CORTEX SEARCH SERVICE {svc_name};\").collect()\n                    if svc_search_col:\n                        svc_search_col = svc_search_col[0][\"search_column\"]\n                    else:\n                        st.warning(f\"No search column found for service {svc_name}\")\n                        continue\n                    # Append service metadata to the list\n                    service_metadata.append({\"name\": svc_name, \"search_column\": svc_search_col})\n\n                except Exception as e:\n                    st.error(f\"Error while fetching metadata for service {svc_name}: {e}\")\n\n        # Store the metadata in session state for later use\n        st.session_state.service_metadata = service_metadata\n\n        # If no services found, show an appropriate message\n        if not service_metadata:\n            st.error(\"No Cortex search services found in Snowflake.\")\n\n\ndef init_config_options():\n    st.sidebar.selectbox(\n        \"Select cortex search service:\",\n        [s[\"name\"] for s in st.session_state.service_metadata],\n        key=\"selected_cortex_search_service\",\n    )\n\n    st.sidebar.button(\"Clear conversation\", key=\"clear_conversation\")\n    st.sidebar.toggle(\"Debug\", key=\"debug\", value=False)\n    st.sidebar.toggle(\"Use chat history\", key=\"use_chat_history\", value=True)\n\n    with st.sidebar.expander(\"Advanced options\"):\n        st.selectbox(\"Select model:\", MODELS, key=\"model_name\")\n        st.number_input(\n            \"Select number of context chunks\",\n            value=500,\n            key=\"num_retrieved_chunks\",\n            min_value=1,\n            max_value=500,\n        )\n        st.number_input(\n            \"Select number of messages to use in chat history\",\n            value=50,\n            key=\"num_chat_messages\",\n            min_value=1,\n            max_value=50,\n        )\n\n    st.sidebar.expander(\"Session State\").write(st.session_state)\n\ndef query_journal_cortex_service(query, columns=[], filter={}):\n    \"\"\"\n    Query the journal cortex search service with the given query and retrieve context documents.\n    Display the retrieved context documents in the sidebar if debug mode is enabled. Return the\n    context documents as a string.\n\n    Args:\n        query (str): The query to search the journal cortex search service with.\n        columns (list): The columns to query in the search service (default is an empty list).\n        filter (dict): The filter conditions for the search (default is an empty dictionary).\n\n    Returns:\n        tuple: A concatenated string of context documents and the results as a list.\n    \"\"\"\n    # Get the active database and schema\n    db, schema = session.get_current_database(), session.get_current_schema()\n\n    # Retrieve the journal Cortex search service\n    cortex_search_service = (\n        root.databases[db]\n        .schemas[schema]\n        .cortex_search_services[\"journal_service\"]  # Use the name of your journal service\n    )\n\n    # Perform the search in the journal cortex search service\n    context_documents = cortex_search_service.search(\n        query, columns=columns, filter=filter, limit=st.session_state.num_retrieved_chunks\n    )\n    results = context_documents.results\n\n    # Concatenate the context documents into a single string\n    context_str = \"\"\n    for i, r in enumerate(results):\n        context_str += f\"Context document {i+1}: {r['chunk']} \\n\" + \"\\n\"\n\n    # If debug mode is enabled, show the context documents in the sidebar\n    if st.session_state.debug:\n        st.sidebar.text_area(\"Context documents\", context_str, height=500)\n\n    return context_str, results\n\n\ndef get_chat_history():\n    \"\"\"\n    Retrieve the chat history from the session state, limited to the number of messages specified\n    by the user in the sidebar options (using the num_chat_messages setting).\n\n    Returns:\n        list: The list of chat messages from the session state.\n    \"\"\"\n    # Determine the start index based on the number of messages specified in the sidebar options\n    start_index = max(\n        0, len(st.session_state.messages) - st.session_state.num_chat_messages\n    )\n    # Return the limited list of chat messages\n    return st.session_state.messages[start_index:]\n\ndef complete(model, prompt):\n    \"\"\"\n    Generate a completion for the given prompt using the specified model.\n\n    Args:\n        model (str): The name of the model to use for completion.\n        prompt (str): The prompt to generate a completion for.\n\n    Returns:\n        str: The generated completion for the journal app's context.\n    \"\"\"\n    # Generate completion using the provided model and prompt, handling special characters like \"$\"\n    response = Complete(model, prompt).replace(\"$\", \"\\$\")\n    \n    # If you need further customization or formatting (like adding references or other elements), \n    # you can modify the returned response here.\n    return response\n\n\ndef make_chat_history_summary(chat_history, question):\n    \"\"\"\n    Generate a summary of the chat history combined with the current question to extend the query\n    context. Use the language model to generate this summary.\n\n    Args:\n        chat_history (str): The chat history to include in the summary.\n        question (str): The current user question to extend with the chat history.\n\n    Returns:\n        str: The generated summary of the chat history and question for the journal app's context.\n    \"\"\"\n    prompt = f\"\"\"\n        [INST]\n        Based on the chat history below and the question, generate a query that extends the question\n        with the chat history provided. The query should be in natural language.\n        Answer with only the query. Do not add any explanation.\n\n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <question>\n        {question}\n        </question>\n        [/INST]\n    \"\"\"\n\n    # Generate the summary using the selected model\n    summary = complete(st.session_state.model_name, prompt)\n\n    # If debug mode is enabled, display the summary in the sidebar for review\n    if st.session_state.debug:\n        st.sidebar.text_area(\n            \"Chat history summary\", summary.replace(\"$\", \"\\$\"), height=150\n        )\n\n    return summary\ndef write_journal_entry():\n    \"\"\"\n    Function to allow users to write their own journal entries\n    and store them in a Snowflake table for future use.\n    \"\"\"\n    st.subheader(\"Write Your Journal Entry\")\n\n    # Input for the journal entry\n    journal_entry = st.text_area(\"Write your thoughts here...\", height=300)\n\n    # Optional title for the entry\n    journal_title = st.text_input(\"Title for your journal entry (optional):\")\n\n    # Button to save the entry\n    if st.button(\"Save Journal Entry\"):\n        if journal_entry.strip() == \"\":\n            st.warning(\"Journal entry cannot be empty!\")\n        else:\n            # Insert the entry into a Snowflake table\n            try:\n                # Get the current date and time\n                timestamp = session.sql(\"SELECT CURRENT_TIMESTAMP;\").collect()[0][0]\n                chunk_size = 1000  # You can define your chunk size\n                chunks = [journal_entry[i:i+chunk_size] for i in range(0, len(journal_entry), chunk_size)]\n\n                for chunk in chunks:\n                    session.sql(\n                        f\"\"\"\n                        INSERT INTO JOURNAL_DB.PUBLIC.JOURNAL_CHUNKS_TABLE(id, chunk, created_at)\n                        VALUES (?, ?, ?)\n                        \"\"\",\n                        [journal_title, chunk, timestamp]\n                    ).collect()\n\n\n                # Insert the data into the Snowflake table\n                session.sql(\n                    f\"\"\"\n                    INSERT INTO journal_entries (title, entry, created_at)\n                    VALUES (?, ?, ?)\n                    \"\"\",\n                    [journal_title, journal_entry, timestamp],\n                ).collect()\n\n                st.success(\"Your journal entry has been saved successfully!\")\n            except Exception as e:\n                st.error(f\"An error occurred while saving your journal entry: {e}\")\n\n\n\ndef create_prompt(user_question):\n    \"\"\"\n    Create a prompt for the language model by combining the user question with context retrieved\n    from the journal cortex search service and chat history (if enabled). Format the prompt according to\n    the expected input format of the model for the journaling app.\n\n    Args:\n        user_question (str): The user's question to generate a prompt for.\n\n    Returns:\n        tuple: The generated prompt for the language model and the search results.\n    \"\"\"\n    if st.session_state.use_chat_history:\n        chat_history = get_chat_history()\n        if chat_history != []:\n            # If there is chat history, summarize it with the current question\n            question_summary = make_chat_history_summary(chat_history, user_question)\n            prompt_context, results = query_journal_cortex_service(\n                question_summary,\n                columns=[\"chunk\"]\n            )\n        else:\n            # If no chat history, query directly based on the user question\n            prompt_context, results = query_journal_cortex_service(\n                user_question,\n                columns=[\"chunk\"]\n            )\n    else:\n        # If chat history is not used, query directly based on the user question\n        prompt_context, results = query_journal_cortex_service(\n            user_question,\n            columns=[\"chunk\"]\n            \n        )\n        chat_history = \"\"  # Empty chat history if not using it\n\n    # Format the context to include timestamps\n    formatted_context = \"\"\n    for i, result in enumerate(results):\n        created_at = result.get(\"created_at\", \"Unknown timestamp\")\n        chunk = result.get(\"chunk\", \"\")\n        formatted_context += f\"Document {i+1} (Created At: {created_at}):\\n{chunk}\\n\\n\"\n\n    # Format the final prompt for the language model\n    prompt = f\"\"\"\n    [INST]\n    You are a helpful AI assistant for a journaling application. When a user asks a question,\n    you will also be given context provided between <context> and </context> tags. Use that context\n    with the user's chat history provided between <chat_history> and </chat_history> tags\n    to provide a meaningful response that answers the user's question. Ensure your response is relevant,\n    empathetic, and concise. If the answer cannot be derived from the provided information, politely inform\n    the user that the information is not available.\n\n    If the user's question is too generic or cannot be answered with the given context or chat_history,\n    respond with \"Sorry, I don't know the answer to that.\"\n\n    <chat_history>\n    {chat_history}\n    </chat_history>\n    <context>\n    {prompt_context}\n    </context>\n    <question>\n    {user_question}\n    </question>\n    [/INST]\n    Answer:\n    \"\"\"\n    return prompt, results \n\n\ndef home_page():\n    \"\"\"\n    New function to display the home page\n    \"\"\"\n    st.markdown(\"\"\"\n        <div style='text-align: center; padding: 2rem;'>\n            <h1 style='color: #9c27b0; margin-bottom: 2rem;'>‚ú® Welcome to Your Digital Journal ‚ú®</h1>\n        </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.markdown(\"\"\"\n            <div style='background: rgba(156, 39, 176, 0.1); padding: 1.5rem; border-radius: 1rem; height: 200px; text-align: center;'>\n                <h3 style='color: #9c27b0;'>‚úç Write</h3>\n                <p>Express your thoughts, feelings, and experiences in your personal digital space.</p>\n            </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    with col2:\n        st.markdown(\"\"\"\n            <div style='background: rgba(156, 39, 176, 0.1); padding: 1.5rem; border-radius: 1rem; height: 200px; text-align: center;'>\n                <h3 style='color: #9c27b0;'>üí≠ Reflect</h3>\n                <p>Review past entries and track your personal growth journey.</p>\n            </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    with col3:\n        st.markdown(\"\"\"\n            <div style='background: rgba(156, 39, 176, 0.1); padding: 1.5rem; border-radius: 1rem; height: 200px; text-align: center;'>\n                <h3 style='color: #9c27b0;'>ü§ñ Chat</h3>\n                <p>Interact with your journal using our AI assistant for deeper insights.</p>\n            </div>\n        \"\"\", unsafe_allow_html=True)\n\ndef main():\n    \"\"\"\n    Enhanced main function with better UI and navigation\n    \"\"\"\n    # Initialize everything as before\n    init_service_metadata()\n    init_config_options()\n    init_messages()\n    \n    # Enhanced navigation with Home page\n    tabs = st.tabs([\"üè† Home\", \"‚úç Write Entry\", \"üí¨ Chat\"])\n    \n    with tabs[0]:\n        home_page()\n    \n    with tabs[1]:\n        write_journal_entry()\n    \n    with tabs[2]:\n        # Your original chat interface with updated styling\n        icons = {\"assistant\": \"üíú\", \"user\": \"üë§\"}\n        \n        # Display existing chat messages\n        for message in st.session_state.messages:\n            with st.chat_message(message[\"role\"], avatar=icons[message[\"role\"]]):\n                st.markdown(message[\"content\"])\n\n        # Chat input logic remains the same\n        disable_chat = (\n            \"service_metadata\" not in st.session_state\n            or len(st.session_state.service_metadata) == 0\n        )\n\n        if question := st.chat_input(\"Ask about your journal...\", disabled=disable_chat):\n            st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n            \n            with st.chat_message(\"user\", avatar=icons[\"user\"]):\n                st.markdown(question.replace(\"$\", \"\\$\"))\n\n            with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):\n                message_placeholder = st.empty()\n                question = question.replace(\"'\", \"\")\n                prompt, results = create_prompt(question)\n\n                with st.spinner(\"Thinking...\"):\n                    generated_response = complete(\n                        st.session_state.model_name, prompt\n                    )\n                    message_placeholder.markdown(generated_response + \"\\n\\n\")\n\n            st.session_state.messages.append(\n                {\"role\": \"assistant\", \"content\": generated_response}\n            )\n\nif __name__ == \"__main__\":\n    session = get_active_session()\n    root = Root(session)\n    main()",
   "execution_count": null,
   "outputs": []
  }
 ]
}